{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing IoT Description files\n"
      ],
      "metadata": {
        "id": "EX1gypFtMhIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Function to split the applicationId and select the part after the first underscore\n",
        "def extract_after_underscore(app_id):\n",
        "    return app_id.split('_')[1] if '_' in app_id else app_id\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "all_metrics_dfs = []\n",
        "\n",
        "# Specify the directory where JSON files are located\n",
        "directory = './Jsonfolder'\n",
        "\n",
        "# Step 2: Read the CSV file into a DataFrame\n",
        "csv_file = 'response-times-dataset.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Iterate over all JSON files in the directory\n",
        "for json_file_path in glob.glob(os.path.join(directory, '*.json')):\n",
        "    # Extract the base name (without extension) from the file path\n",
        "    base_name = os.path.splitext(os.path.basename(json_file_path))[0]\n",
        "\n",
        "    # Print the base name\n",
        "    print(base_name)\n",
        "\n",
        "    # Load JSON data from the file\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Convert IoT Devices to DataFrame\n",
        "    iot_devices = pd.DataFrame(data['IoTdevices'])\n",
        "    iot_devices['publishesTo'] = iot_devices['publishesTo'].apply(lambda x: ';'.join(x))\n",
        "\n",
        "    # Convert Applications to DataFrame\n",
        "    applications = pd.DataFrame(data['applications'])\n",
        "    applications['subscribesTo'] = applications['subscribesTo'].apply(lambda x: ';'.join(x))\n",
        "    # applications['applicationCategory'] = applications['applicationCategory']\n",
        "\n",
        "\n",
        "    # Convert System Configuration to DataFrame\n",
        "    system_config = {\n",
        "        'systemBandwidth': [data['systemBandwidth']],\n",
        "        'bandwidthPolicy': [data['bandwidthPolicy']],\n",
        "        'priorityPolicy': [data['priorityPolicy']],\n",
        "        'commChannelLossAN': [data['commChannelLossAN']],\n",
        "        'commChannelLossRT': [data['commChannelLossRT']],\n",
        "        'commChannelLossTS': [data['commChannelLossTS']],\n",
        "        'commChannelLossVS': [data['commChannelLossVS']],\n",
        "        'brokerCapacity': [data['brokerCapacity']]\n",
        "    }\n",
        "    system_config_df = pd.DataFrame(system_config)\n",
        "\n",
        "    # Extract the required columns from the CSV DataFrame\n",
        "    columns_of_interest = ['topic', 'app', base_name]\n",
        "\n",
        "    # Filter out columns that do not exist in the DataFrame\n",
        "    columns_of_interest = [col for col in columns_of_interest if col in df.columns]\n",
        "\n",
        "    # Create a DataFrame with the filtered columns\n",
        "    df_filtered = df[columns_of_interest].copy() if columns_of_interest else pd.DataFrame()\n",
        "\n",
        "    if df_filtered.empty:\n",
        "        print(f\"No columns of interest found in {csv_file}. Skipping this file.\")\n",
        "        continue\n",
        "\n",
        "    # Add a key for cross join\n",
        "    df_filtered['key'] = 1\n",
        "    system_config_df['key'] = 1\n",
        "\n",
        "    # Merge the DataFrames using the cross join approach\n",
        "    merged_df = pd.merge(df_filtered, system_config_df, on='key').drop('key', axis=1)\n",
        "\n",
        "    # Apply the function to the applicationId column\n",
        "    applications['applicationId'] = applications['applicationId'].apply(extract_after_underscore)\n",
        "\n",
        "    # Perform the merge with applications DataFrame\n",
        "    result_df = pd.merge(merged_df, applications, left_on='app', right_on='applicationId', how='left')\n",
        "\n",
        "    # Apply the function to the publishesTo column\n",
        "    iot_devices['publishesTo'] = iot_devices['publishesTo'].apply(extract_after_underscore)\n",
        "\n",
        "    # Perform the merge with iot_devices DataFrame\n",
        "    final_df = pd.merge(result_df, iot_devices, left_on='topic', right_on='publishesTo', how='left')\n",
        "\n",
        "    # Create a new column 'priorityID' and populate it with the base_name\n",
        "    final_df['priorityID'] = base_name\n",
        "\n",
        "    final_df['scenario_case'] = 100\n",
        "\n",
        "    # Select and rename columns\n",
        "    columns_to_select = ['topic', 'app', 'applicationCategory', base_name, 'systemBandwidth', 'commChannelLossAN',\n",
        "                          'commChannelLossRT', 'commChannelLossTS', 'commChannelLossVS', 'brokerCapacity',\n",
        "                          'priority', 'priorityID', 'processingRate', 'publishFrequency', 'messageSize', 'scenario_case']\n",
        "\n",
        "    # Filter out columns that do not exist in the DataFrame\n",
        "    columns_to_select = [col for col in columns_to_select if col in final_df.columns]\n",
        "\n",
        "    final_df_filtered = final_df[columns_to_select] if columns_to_select else pd.DataFrame()\n",
        "\n",
        "    if final_df_filtered.empty:\n",
        "        print(f\"No columns to select in the final DataFrame. Skipping this iteration.\")\n",
        "        continue\n",
        "\n",
        "    final_df_filtered = final_df_filtered.rename(columns={base_name: 'latency'})\n",
        "\n",
        "    # Append the final DataFrame to the list\n",
        "    all_metrics_dfs.append(final_df_filtered)\n",
        "\n",
        "# Concatenate all DataFrames in the list into one DataFrame\n",
        "if all_metrics_dfs:\n",
        "    metrics_df = pd.concat(all_metrics_dfs, ignore_index=True)\n",
        "    # Print or save the DataFrame\n",
        "    print(metrics_df.head(2))\n",
        "else:\n",
        "    print(\"No dataframes to concatenate.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeQNfeqsE7im",
        "outputId": "ff5d5190-f740-4821-d926-19e9910c89f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dropVS15AN15\n",
            "dropVS10\n",
            "prioRTVSTSAN\n",
            "prioRT\n",
            "dropVS10AN10RT10\n",
            "dropRT10\n",
            "prioTS\n",
            "prioRTVS\n",
            "baseline\n",
            "prioritizeTopics\n",
            "maxmin\n",
            "prioVS\n",
            "prioAN\n",
            "dropVS10AN10\n",
            "dropAN10\n",
            "plannerConfiguration\n",
            "        topic    app applicationCategory   latency  systemBandwidth  \\\n",
            "0  amazonecho  app10                  AN  5.406390              650   \n",
            "1  amazonecho  app14                  TS  5.252253              650   \n",
            "\n",
            "   commChannelLossAN  commChannelLossRT  commChannelLossTS  commChannelLossVS  \\\n",
            "0               0.15                0.0                  0               0.15   \n",
            "1               0.15                0.0                  0               0.15   \n",
            "\n",
            "   brokerCapacity  priority    priorityID  processingRate  publishFrequency  \\\n",
            "0             100         0  dropVS15AN15            1000                 1   \n",
            "1             100         0  dropVS15AN15            1000                 1   \n",
            "\n",
            "   messageSize  scenario_case  \n",
            "0    105869.28            100  \n",
            "1    105869.28            100  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(metrics_df))\n",
        "\n",
        "metrics_df.to_csv('100Subs_metrics_df.csv', index=False)"
      ],
      "metadata": {
        "id": "zTHKh43NGbF5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fe13ef-755c-478b-b294-28e39801c220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6kx4YNPGMMeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concat for Scenario 3 - Scalabiltiy\n"
      ],
      "metadata": {
        "id": "VPpy2KQEf5Xh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# concat_files = ['100Subs_metrics_df.csv', '80Subs_metrics_df.csv', '60Subs_metrics_df.csv', '40Subs_metrics_df.csv', '20Subs_metrics_df.csv']\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# List of file names\n",
        "concat_files = [\n",
        "    '100Subs_metrics_df.csv',\n",
        "    '80Subs_metrics_df.csv',\n",
        "    '60Subs_metrics_df.csv',\n",
        "    '40Subs_metrics_df.csv',\n",
        "    '20Subs_metrics_df.csv'\n",
        "]\n",
        "\n",
        "# Read and concatenate all files\n",
        "dfs = [pd.read_csv(file) for file in concat_files]\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Optionally, save the combined DataFrame to a new CSV file\n",
        "combined_df.to_csv('combined_metrics_df.csv', index=False)\n",
        "\n",
        "# Print the first few rows of the combined DataFrame\n",
        "print(combined_df.head(1))\n",
        "\n"
      ],
      "metadata": {
        "id": "MCGjJt0dKBF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4f36d3-b553-4785-8714-b79d3b2ea12f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        topic    app applicationCategory   latency  systemBandwidth  \\\n",
            "0  amazonecho  app10                  AN  5.406390              650   \n",
            "1  amazonecho  app14                  TS  5.252253              650   \n",
            "2  amazonecho  app19                  TS  5.247047              650   \n",
            "3  amazonecho  app21                  RT  5.344094              650   \n",
            "4  amazonecho  app27                  RT  5.378431              650   \n",
            "\n",
            "   commChannelLossAN  commChannelLossRT  commChannelLossTS  commChannelLossVS  \\\n",
            "0               0.15                0.0                  0               0.15   \n",
            "1               0.15                0.0                  0               0.15   \n",
            "2               0.15                0.0                  0               0.15   \n",
            "3               0.15                0.0                  0               0.15   \n",
            "4               0.15                0.0                  0               0.15   \n",
            "\n",
            "   brokerCapacity  priority    priorityID  processingRate  publishFrequency  \\\n",
            "0             100         0  dropVS15AN15            1000                 1   \n",
            "1             100         0  dropVS15AN15            1000                 1   \n",
            "2             100         0  dropVS15AN15            1000                 1   \n",
            "3             100         0  dropVS15AN15            1000                 1   \n",
            "4             100         0  dropVS15AN15            1000                 1   \n",
            "\n",
            "   messageSize  scenario_case  \n",
            "0    105869.28            100  \n",
            "1    105869.28            100  \n",
            "2    105869.28            100  \n",
            "3    105869.28            100  \n",
            "4    105869.28            100  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HU23yuewMWaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete folders\n"
      ],
      "metadata": {
        "id": "1tv3lf04fzqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def delete_folder_contents(folder_path):\n",
        "    # Check if the folder exists\n",
        "    if os.path.exists(folder_path):\n",
        "        # Iterate over all files and folders in the directory\n",
        "        for item in os.listdir(folder_path):\n",
        "            item_path = os.path.join(folder_path, item)\n",
        "            # Check if it's a file or directory and delete accordingly\n",
        "            if os.path.isfile(item_path):\n",
        "                os.remove(item_path)\n",
        "            elif os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "    else:\n",
        "        print(f\"Folder '{folder_path}' does not exist.\")\n",
        "\n",
        "folder_path = './Jsonfolder'\n",
        "# folder_path = './'\n",
        "delete_folder_contents(folder_path)\n"
      ],
      "metadata": {
        "id": "K_EHJYDGJUpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8rHq-ohItmz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}