# -*- coding: utf-8 -*-
"""CausalDiscovery_PC_v7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_bby8YLJvX4Dg7gNW44nq4PHOtjv0DsM

### CAUSAL DISCOVERY - PETER CLARK

 * This notebook is used for testing the Peter-Clark Algorithm for Causal Discovery using Py-Why
 * The Peter-Clark Algorithm is a Constraint-Based Algorithm method for inferring causal relationships from observational data.
 * It is commonly used in the field of causal inference and can help identify cause-effect relationships between variables in a dataset.
 * For more information on the Peter-Clark Algorithm and how to get started with causal inference, please refer to the documentation at: [https://causal-learn.readthedocs.io/en/latest/getting_started.html](https://causal-learn.readthedocs.io/en/latest/getting_started.html)
"""

# !pip install -r requirement.txt

import os, time, shutil
import zipfile
import os
import sys
sys.path.append("")
import unittest
import hashlib
import numpy as np
from causallearn.search.ConstraintBased.PC import pc
from causallearn.utils.cit import chisq, fisherz, gsq, kci, mv_fisherz, d_separation
from causallearn.graph.SHD import SHD
from causallearn.utils.DAG2CPDAG import dag2cpdag
from causallearn.utils.TXT2GeneralGraph import txt2generalgraph
import networkx as nx
import matplotlib.pyplot as plt
import pandas as pd
import pickle
# from .utils_simulate_data import simulate_discrete_data, simulate_linear_continuous_data
from causallearn.graph.GraphClass import CausalGraph
from causallearn.graph.GraphNode import GraphNode
from causallearn.utils.cit import fisherz, mv_fisherz
from causallearn.utils.PCUtils import SkeletonDiscovery
from causallearn.utils.PCUtils.BackgroundKnowledge import BackgroundKnowledge
from causallearn.utils.PCUtils.BackgroundKnowledgeOrientUtils import \
    orient_by_background_knowledge

# or save the graph
from causallearn.utils.GraphUtils import GraphUtils


import gc
gc.collect()


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~IMPORTANT~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Extract the zip file and rename the CSV file from the preprocessing
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Insert the file in the ./data directory and rename it as "extract.csv"
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#To select a list of columns to use, edit the select_list variable. Response time would be node 0
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



# """### LOADING DATASET"""
#IF ZIP FILE

# # Define the path to the zip file
# zip_file_path = '/content/final_results_10s.zip'

# # Define the directory path to extract the files
# extracted_dir_path = '/content/data/'

# # Create the directory if it doesn't exist
# os.makedirs(extracted_dir_path, exist_ok=True)

# # Extract the zip file
# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
#     zip_ref.extractall(extracted_dir_path)

# # Get the list of files extracted
# extracted_files = zip_ref.namelist()

# # Find the CSV file in the extracted files
# csv_file = [file for file in extracted_files if file.endswith('.csv')]

# if csv_file:
#     # Rename the CSV file to 'output.csv'
#     os.rename(os.path.join(extracted_dir_path, csv_file[0]), os.path.join(extracted_dir_path, 'extract.csv'))
#     print('CSV file extracted and renamed successfully.')
# else:
#     print('No CSV file found in the zip file.')

# Define paths
output_csv_path = "./data/output.csv"
output_p_path = "./data/output.p"
output_txt_path = "./data/output.txt"
cache_path = "./cache"


# # Define a function to sample each group
# def sample_group(group):
#     if len(group) < num_samples_per_group:
#         return group  # If the group size is smaller than the desired number of samples, return all samples
#     else:
#         return group.sample(n=num_samples_per_group, replace=False)  # Sample the specified number of samples from the group


# Define files to be deleted
files_to_delete = [output_csv_path,
                   output_p_path,
                   output_txt_path]

# Delete files if they exist
for file_path in files_to_delete:
    if os.path.exists(file_path):
        os.remove(file_path)

# Delete the cache directory if it exists
if os.path.exists(cache_path):
    shutil.rmtree(cache_path)
    print(f"Directory '{cache_path}' deleted successfully.")
else:
    print(f"Directory '{cache_path}' does not exist.")


fpth = "./data/extract.csv"
spth = "./data/output.csv"

# Define the columns you want to select
select_list = ['responsetime', 'Bandwidth', 'sensorid', 'cpuUsage', 'freeMemory', 'freeSpace', 'Devices_Sum', 'Virtualsensors_Sum', 'Applications_Sum', 'cpu_quota']


# Read the CSV file into a DataFrame and select the desired columns
slctd_df = pd.read_csv(fpth, usecols=select_list)

# Reorder the columns in the DataFrame
slctd_df = slctd_df[select_list]

# # Assuming 'sensorid' is an important categorical variable
# num_samples_per_group = 300  # Specify the number of samples you want per group

# samp_by = 'Bandwidth' # Specify group by
# # samp_by = 'day' # Specify group by

# Apply the sampling function to each group
# slctd_df = slctd_df.groupby(samp_by, group_keys=False).apply(sample_group)


print("Number of rows:", slctd_df.shape[0])
print("Number of columns:", slctd_df.shape[1])


# Save the selected DataFrame to another CSV file
slctd_df.to_csv(spth, index=False)


# Load data
def load_data_save_to_txt():
    """
    Load data from either a pickle file or a CSV file and save it to a text file.

    Returns:
        pandas.DataFrame: The loaded data.
    """
    pickle_file_path = "./data/output.p"
    csv_file_path = "./data/output.csv"
    txt_file_path = './data/output.txt'

    if os.path.exists(pickle_file_path):
        df = pd.read_pickle(pickle_file_path)
    else:
        df = pd.read_csv(csv_file_path)

        df.to_pickle(pickle_file_path, protocol=pickle.HIGHEST_PROTOCOL)

    df.to_csv(txt_file_path, sep='\t', index=False)

    return df

# View the first few rows of the data
try:
    df = load_data_save_to_txt()
    df.head()
except Exception as e:
    print(f"Csv or Pickle file not found")

## Load data for Causal Discovery from a txt file
def load_txt_dag_data(data_path, num_rows):
    """
    Load data from a text file.

    Args:
        data_path (str): Path to the data file.
        num_rows (int): Number of rows to load from the data file. If None, all rows will be loaded.

    Returns:
        numpy.ndarray: The loaded data as a numpy array.
    """
    full_data = np.loadtxt(data_path, skiprows=1)

    # Randomly select num_rows rows
    if num_rows is not None:
        indices = np.random.choice(full_data.shape[0], num_rows, replace=False)
        data = full_data[indices, :]
    else:
        data = full_data

    return data

## Load Ground Truth Directed Acyclic Graph (DAG)
def load_truth_dag(truth_graph_path):
    """
    Load the ground truth directed acyclic graph (DAG) from a text file.

    Args:
        truth_graph_path (str): Path to the truth graph file.

    Returns:
        Graph: The loaded truth DAG.
    """

    truth_dag = txt2generalgraph(truth_graph_path)
    return truth_dag


# Usage example:
# Load data
data_path = "./data/output.txt"   # "output.txt"
# truth_graph_path = "./data/graph.10.txt"
num_rows = None #select number of rows to load or None

data = load_txt_dag_data(data_path, num_rows)

# txt_file_path = 'your_file.txt'

# Read the first line of the text file
with open(data_path, 'r') as file:
    first_line = file.readline().strip()  # Read the first line and remove leading/trailing whitespace

# Assuming the header is separated by commas, split the line based on the delimiter
header = first_line.split('\t')

# Print or use the header
print(header)


"""### RUNNING ALGORITHM"""

def run_causal_discovery(data):
    """
    Runs the modified PC algorithm for causal discovery on the given data.

    Parameters:
    - data: The input data for causal discovery.
    - alpha: The significance level for conditional independence tests.
    - indep_test: The independence test to use.
        # “fisherz”, “chisq”, “gsq”, “kci”, “mv_fisherz”
    - stable: Whether to use the stable PC algorithm.
    - uc_rule: The unshielded colliders rule to use.
    - uc_priority: The priority of unshielded colliders.
    - mvpc: Whether to use the modified v-structure phase condition.
    - correction_name: The name of the correction to use.
    - background_knowledge: Background knowledge to incorporate in the causal discovery.
    - verbose: Whether to print verbose output.
    - show_progress: Whether to show progress during the algorithm execution.

    Returns:
    - cg: The causal graph object representing the discovered causal relationships.

    Example usage:
    cg = run_causal_discovery(data, 0.05, kci, stable=True, uc_rule=1, uc_priority=1, mvpc=True, correction_name='fdr_bh', background_knowledge=None, verbose=True, show_progress=True)
    """
    # cg = pc(data, alpha, indep_test, stable, uc_rule, uc_priority, mvpc, correction_name, background_knowledge, verbose, show_progress)

    # default parameters
    start_time = time.time()

    citest_cache_file = "./cache/citest_cache_kci.json"    # .json file
    cg = pc(data, 0.1, fisherz, cache_path=citest_cache_file, uc_rule=1, uc_priority=1)             # after the long run

    # cg = pc(data, 0.05, kci, cache_path=citest_cache_file)


    # visualization using pydot
    cg.draw_pydot_graph(labels=header)

    # save the graph
    pyd = GraphUtils.to_pydot(cg.G, labels=header)
    pyd.write_png('PC_graph_test.png')

    # # # visualization using networkx
    # cg.to_nx_graph()
    # cg.draw_nx_graph(skel=False)

    end_time = time.time()
    time_taken = round((end_time - start_time), 3)
    print(f"Time taken for PC algorithm: {time_taken} seconds")

    return cg


"""### Without Background Knowledge"""
# Usage example:
# Run the modified PC algorithm for causal discovery
# cg = run_causal_discovery(data)

# ‘cg.draw_pydot_graph(labels=[“A”, “B”, “C”])’




"""### Using Background Knowledge"""
# ~~~~~STARTS HERE~~~~~~

## Using Background Knowledge
start_time = time.time()

citest_cache_file = "./cache/citest_cache_fisherz.json"
cg_without_background_knowledge = pc(data, 0.05, fisherz, True, cache_path=citest_cache_file, uc_rule=1, uc_priority=4)
nodes = cg_without_background_knowledge.G.get_nodes()

# visualization using pydot
cg_without_background_knowledge.draw_pydot_graph(labels=header)

# save the graph
pyd = GraphUtils.to_pydot(cg_without_background_knowledge.G, labels=header)
pyd.write_png('PC_graph_test_without_bk.png')



bk = BackgroundKnowledge() \
    .add_forbidden_by_node(nodes[0], nodes[1]).add_forbidden_by_node(nodes[0], nodes[2])\
    .add_forbidden_by_node(nodes[0], nodes[3]).add_forbidden_by_node(nodes[0], nodes[4])\
    .add_forbidden_by_node(nodes[0], nodes[5]).add_forbidden_by_node(nodes[0], nodes[6])\
    .add_forbidden_by_node(nodes[0], nodes[7]).add_forbidden_by_node(nodes[0], nodes[8])\
    .add_forbidden_by_node(nodes[0], nodes[9])
    

    

citest_cache_file_bk = "./cache/citest_cache_fisherz_bk.json"    # .json file

#Edit the parameters as needed 
cg_with_background_knowledge = pc(data, 0.05, fisherz, True, background_knowledge=bk, cache_path=citest_cache_file_bk, uc_rule=1, uc_priority=4)
# cg_with_background_knowledge = pc(data, 0.1, kci, True, uc_rule=1, uc_priority=1, cache_path=citest_cache_file_bk)

# visualization using pydot
cg_with_background_knowledge.draw_pydot_graph(labels=header)

# save the graph
pyd = GraphUtils.to_pydot(cg_with_background_knowledge.G, labels=header)
pyd.write_png('PC_graph_test_bk_sysfish.png')

end_time = time.time()
time_taken = round((end_time - start_time), 3)
print(f"Time taken for PC algorithm: {time_taken} seconds")



# Extract the learned causal graph
# learned_graph = cg.G
learned_graph = cg_with_background_knowledge.G

# Print or use the learned graph
# print(learned_graph)

# Define the path where the learned graph will be saved
txt_file_path = 'PC_learned_graph_bk.txt'

# Define the path for the backup copy
backup_file_path = 'PC_learned_graph_orig.txt'

# Save the learned graph to a text file in truth format
with open(txt_file_path, 'w') as file:
    file.write(str(learned_graph))

# Copy the original file to create a backup
shutil.copyfile(txt_file_path, backup_file_path)

# print(f'Learned graph saved to {txt_file_path}.')

# Read the content of the text file
with open(txt_file_path, 'r') as file:
    graph_output = file.read()

# Iterate through the header list and replace placeholders with variable names
for i, head in enumerate(header):
    graph_output = graph_output.replace(f'X{i+1}', head)

# Save the modified content back to the same file
with open(txt_file_path, 'w') as file:
    file.write(graph_output)
    print(graph_output)

print(f'Modified graph content saved back to {txt_file_path}.')











